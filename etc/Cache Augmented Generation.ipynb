{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILED with MS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "-ASdoipcwGyR",
   "metadata": {
    "id": "-ASdoipcwGyR"
   },
   "source": [
    "### Installing Important Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ylo_gh42Fm2p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5632,
     "status": "ok",
     "timestamp": 1736436156672,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "ylo_gh42Fm2p",
    "outputId": "6d4acd1f-0a7e-43b3-9cc4-eaa828cbff5a"
   },
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "izQNoPBPv7SU",
   "metadata": {
    "id": "izQNoPBPv7SU"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sPZdUF6Wv6_a",
   "metadata": {
    "id": "sPZdUF6Wv6_a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0591671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f6b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"B:\\\\Work\\\\Code\\\\f\\\\4\\\\testing\\\\llm\\\\models\\\\unsloth-Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcHsquxWGLdB",
   "metadata": {
    "id": "mcHsquxWGLdB"
   },
   "source": [
    "### Generate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1IqOLKJvEfVa",
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1736436973552,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "1IqOLKJvEfVa"
   },
   "outputs": [],
   "source": [
    "# Minimal generate function for token-by-token generation\n",
    "def generate(model, input_ids: torch.Tensor, past_key_values, max_new_tokens: int = 50) -> torch.Tensor:\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    origin_len = input_ids.shape[-1]\n",
    "    input_ids = input_ids.to(device)\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model(\n",
    "                input_ids=next_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = out.logits[:, -1, :]\n",
    "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            output_ids = torch.cat([output_ids, token], dim=-1)\n",
    "            past_key_values = out.past_key_values\n",
    "            next_token = token.to(device)\n",
    "\n",
    "            if model.config.eos_token_id is not None and token.item() == model.config.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Return just the newly generated part\n",
    "    return output_ids[:, origin_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBD6NNltGP9O",
   "metadata": {
    "id": "ZBD6NNltGP9O"
   },
   "source": [
    "### Dynamic Cache Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "UpXP3xKpEfVb",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1736436282535,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "UpXP3xKpEfVb"
   },
   "outputs": [],
   "source": [
    "# Initializing the DynamicCache mechanism for storing and reusing the modelâ€™s key/value states.\n",
    "torch.serialization.add_safe_globals([DynamicCache])\n",
    "torch.serialization.add_safe_globals([set])\n",
    "\n",
    "def get_kv_cache(model, tokenizer, prompt: str) -> DynamicCache:\n",
    "    # Encode prompt\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    cache = DynamicCache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return cache\n",
    "\n",
    "# Remove any extra tokens appended by user queries, appended to the original knowledge\n",
    "def clean_up(cache: DynamicCache, origin_len: int):\n",
    "    for i in range(len(cache.key_cache)):\n",
    "        cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n",
    "        cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5diLXLveEfVc",
   "metadata": {
    "id": "5diLXLveEfVc"
   },
   "source": [
    "### Load LLM Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3FdvEEMEfVc",
   "metadata": {
    "collapsed": true,
    "id": "f3FdvEEMEfVc"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    # model_name = \"../model/bitnet-b1.58-2B-4T\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"cuda\", # <==== TODO change this\n",
    "            )\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer,model = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cV6N8TdEEfVc",
   "metadata": {
    "id": "cV6N8TdEEfVc"
   },
   "source": [
    "### Create a Knowledge Base from input file and prepare KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfsUydzrEfVc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5017,
     "status": "ok",
     "timestamp": 1736436923290,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "bfsUydzrEfVc",
    "outputId": "7fa36eb4-f84d-4427-9f2e-0fe2ce895cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.47 GiB is allocated by PyTorch, and 92.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.47 GiB is allocated by PyTorch, and 92.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Specify file path and prepare KV cache\u001b[39;00m\n\u001b[0;32m     35\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/text_1.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 36\u001b[0m kV_cache,origin_len \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_system_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mprepare_system_prompt\u001b[1;34m(file_path, model, tokenizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m<|system|>\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mYou are an assistant who provides concise factual answers.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build and return KV cache\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_kv_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m origin_len \u001b[38;5;241m=\u001b[39m kv_cache\u001b[38;5;241m.\u001b[39mkey_cache[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKV cache built. Original length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morigin_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mget_kv_cache\u001b[1;34m(model, tokenizer, prompt)\u001b[0m\n\u001b[0;32m      9\u001b[0m cache \u001b[38;5;241m=\u001b[39m DynamicCache()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[0;32m    841\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[1;32m--> 842\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.47 GiB is allocated by PyTorch, and 92.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def prepare_system_prompt(file_path, model, tokenizer):\n",
    "    try:\n",
    "        # Ensure the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}. Please create a file with the necessary context.\")\n",
    "\n",
    "        # Read content from the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            input_text = f.read().strip()\n",
    "\n",
    "        # Create the system prompt\n",
    "        system_prompt = f\"\"\"\n",
    "        <|system|>\n",
    "        You are an assistant who provides concise factual answers.\n",
    "        <|user|>\n",
    "        Context:\n",
    "        {input_text}\n",
    "        \"\"\".strip()\n",
    "\n",
    "        # Build and return KV cache\n",
    "        kv_cache = get_kv_cache(model, tokenizer, system_prompt)\n",
    "        origin_len = kv_cache.key_cache[0].shape[-2]\n",
    "        print(f\"KV cache built. Original length: {origin_len}\")\n",
    "        return kv_cache,origin_len\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Specify file path and prepare KV cache\n",
    "file_path = \"../input/text_1.txt\"\n",
    "kV_cache,origin_len = prepare_system_prompt(file_path, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQjC-WjSEfVc",
   "metadata": {
    "id": "VQjC-WjSEfVc"
   },
   "source": [
    "### Ask Questions Reusing the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JZos3PhoEfVc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1736437731608,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "JZos3PhoEfVc",
    "outputId": "e2c455bd-7832-46e9-e995-23a5c8ef0a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What caused the rain's taste?\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "CPU times: total: 5min 25s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1st query\n",
    "question1 = \"What caused the rain's taste?\"\n",
    "clean_up(kV_cache, origin_len)\n",
    "input_ids_q1 = tokenizer(question1 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q1 = generate(model, input_ids_q1, kV_cache)\n",
    "answer1 = tokenizer.decode(gen_ids_q1[0], skip_special_tokens=True)\n",
    "print(\"Q1:\", question1)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7lzogfXzEfVd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6392,
     "status": "ok",
     "timestamp": 1736437064115,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "7lzogfXzEfVd",
    "outputId": "ecb5b4b9-0cfc-46a0-ee24-0463e5c06025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: What is Aerilon?\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "CPU times: total: 5min 19s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2nd query\n",
    "question2 = \"What is Aerilon?\"\n",
    "clean_up(kV_cache, origin_len)\n",
    "input_ids_q2 = tokenizer(question2 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q2 = generate(model, input_ids_q2, kV_cache)\n",
    "answer2 = tokenizer.decode(gen_ids_q2[0], skip_special_tokens=True)\n",
    "print(\"Q2:\", question2)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fqrtyyO0wV6i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7979,
     "status": "ok",
     "timestamp": 1736437878739,
     "user": {
      "displayName": "SHIB KUMAR SARAF",
      "userId": "11784079462216008979"
     },
     "user_tz": -330
    },
    "id": "fqrtyyO0wV6i",
    "outputId": "94e960e9-fc5c-4e7d-8bd1-67326a659cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: What resists understanding?\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "CPU times: total: 5min 23s\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3rd query\n",
    "question3 = \"What resists understanding?\"\n",
    "clean_up(kV_cache, origin_len)\n",
    "input_ids_q3 = tokenizer(question3 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q3 = generate(model, input_ids_q3, kV_cache)\n",
    "answer3 = tokenizer.decode(gen_ids_q3[0], skip_special_tokens=True)\n",
    "print(\"Q3:\", question3)\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657983f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
