{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d1e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869399",
   "metadata": {},
   "source": [
    "# non cached version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"../model/bitnet-b1.58-2B-4T\"\n",
    "# model_id = \"B:\\\\Work\\\\Code\\\\f\\\\4\\\\testing\\\\llm\\\\models\\\\unsloth-Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d9f3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant Response: The rain in Veridia tastes faintly of iron due to the constant, low-level hum emanating from the Chronarium, a structure dominating the western cliffs overlooking the city. This hum is not due to the mineral content of the soil, although it\n",
      "CPU times: total: 3min 15s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply the chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": '''\n",
    "     Answer the question based on context\n",
    "     The rain in Veridia always tasted faintly of iron. This wasn't due to the mineral content of the soil \\u2013 though it was unusually high \\u2013 but to the constant, low-level hum emanating from the Chronarium, a structure dominating the western cliffs overlooking the city. It\\u2019s a place I\\u2019ve spent my entire life studying, and one that remains stubbornly resistant to complete understanding, even after decades of research. \"\n",
    "     '''},\n",
    "    {\"role\": \"user\", \"content\": \"What caused the rain's taste?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "chat_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "chat_outputs = model.generate(**chat_input, max_new_tokens=50)\n",
    "response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True) # Decode only the response part\n",
    "print(\"\\nAssistant Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f6ff7",
   "metadata": {},
   "source": [
    "# CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686f3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import os\n",
    "import logging \n",
    "from typing import Union\n",
    "from util import get_training_data\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5a2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_knowledge(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     prompt: str,\n",
    "# ) -> DynamicCache:\n",
    "#     \"\"\"\n",
    "#     Prepare knowledge kv cache for CAG.\n",
    "#     Args:\n",
    "#         model: HuggingFace model with automatic device mapping\n",
    "#         tokenizer: HuggingFace tokenizer\n",
    "#         prompt: The knowledge to preprocess, which is basically a prompt\n",
    "\n",
    "#     Returns:\n",
    "#         DynamicCache: KV Cache\n",
    "#     \"\"\"\n",
    "#     embed_device = model.model.embed_tokens.weight.device\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "#     past_key_values = DynamicCache()\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             past_key_values=past_key_values,\n",
    "#             use_cache=True,\n",
    "#             output_attentions=False,\n",
    "#             output_hidden_states=False\n",
    "#         )\n",
    "#     return outputs.past_key_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abab3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f895b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_kv_cache(kv: DynamicCache, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \"\"\"\n",
    "    Write the KV Cache to a file.\n",
    "    \"\"\"\n",
    "    torch.save(kv, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_kvcache(documents, filepath: str = \"./cache/cache_knowledges.pt\", answer_instruction: Union[str , None] = None):\n",
    "#     # Prepare the knowledges kvcache\n",
    "\n",
    "#     if answer_instruction is None:\n",
    "#         answer_instruction = \"Answer the question with a super short answer.\"\n",
    "#     knowledges = f\"\"\"\n",
    "#     <|begin_of_text|>\n",
    "#     <|start_header_id|>system<|end_header_id|>\n",
    "#     You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "#     <|start_header_id|>user<|end_header_id|>\n",
    "#     Context information is bellow.\n",
    "#     ------------------------------------------------\n",
    "#     {documents}\n",
    "#     ------------------------------------------------\n",
    "#     {answer_instruction}\n",
    "#     Question:\n",
    "#     \"\"\"\n",
    "#     # Get the knowledge cache\n",
    "#     t1 = time()\n",
    "#     model_name = \"../model/bitnet-b1.58-2B-4T\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,\n",
    "#             device_map=\"cpu\", # <==== TODO change this\n",
    "            \n",
    "#         )\n",
    "#     kv = preprocess_knowledge(model, tokenizer, knowledges)\n",
    "#     print(\"kvlen: \", kv.key_cache[0].shape[-2])\n",
    "#     write_kv_cache(kv, filepath)\n",
    "#     t2 = time()\n",
    "#     logger.info(f\"KV cache prepared in {t2 - t1:.2f} seconds.\")\n",
    "#     return kv, t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c193a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(kv: DynamicCache, origin_len: int):\n",
    "    \"\"\"\n",
    "    Truncate the KV Cache to the original length.\n",
    "    \"\"\"\n",
    "    for i in range(len(kv.key_cache)):\n",
    "        kv.key_cache[i] = kv.key_cache[i][:, :, :origin_len, :]\n",
    "        kv.value_cache[i] = kv.value_cache[i][:, :, :origin_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa2b1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 300\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: KV Cache for knowledge\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "    \"\"\"\n",
    "\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=next_token, \n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            next_token = next_token.to(embed_device)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "            # if next_token.item() in model.config.eos_token_id:\n",
    "            #     break\n",
    "    return output_ids[:, origin_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b7854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_instruction = \"Answer the asked question.\"\n",
    "# text_list, dataset = get_training_data(\"../input/training_data_1.json\")\n",
    "# kvcache_path = \"./data_cache/cache_knowledges.pt\"\n",
    "# knowledges = '\\n\\n\\n\\n\\n\\n'.join(text_list)\n",
    "# knowledge_cache, prepare_time = prepare_kvcache(knowledges, filepath=kvcache_path, answer_instruction=answer_instruction)\n",
    "# kv_len = knowledge_cache.key_cache[0].shape[-2]\n",
    "# print(f\"KVcache prepared in {prepare_time} seconds\")\n",
    "# with open(\"./output/result_2.txt\", \"a\") as f:\n",
    "#     f.write(f\"KVcache prepared in {prepare_time} seconds\\n\")\n",
    "\n",
    "# results = {\n",
    "#     \"cache_time\": [],\n",
    "#     \"generate_time\": [],\n",
    "#     \"similarity\": [],\n",
    "#     \"prompts\": [],\n",
    "#     \"responses\": []\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "274b373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kv_cache(path: str) -> Union[DynamicCache , None]:\n",
    "    \"\"\"\n",
    "    Read the KV Cache from a file. If the cache file is invalid or empty, return None.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        kv = torch.load(path, weights_only=False)\n",
    "        return kv\n",
    "    else:\n",
    "        # Regenerate cache if it doesn't exist or is too small\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d647bcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  What caused the rain's taste?\n",
      "A:  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1745052371.0781054\n",
      "Q:  Where is the structure located?\n",
      "A:  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1745052695.5691574\n",
      "Q:  What resists understanding?\n",
      "A:  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1745053018.9888854\n",
      "Q:  What is Aerilon?\n",
      "A:  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1745053343.976633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:62\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(model, input_ids, past_key_values, max_new_tokens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m---> 27\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     33\u001b[0m         next_token \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\models\\bitnet\\modeling_bitnet.py:813\u001b[0m, in \u001b[0;36mBitNetForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    809\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    810\u001b[0m )\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 813\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    814\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    815\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    816\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    817\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    818\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    819\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    820\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    821\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    822\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    824\u001b[0m )\n\u001b[0;32m    826\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    827\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\models\\bitnet\\modeling_bitnet.py:571\u001b[0m, in \u001b[0;36mBitNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    561\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m         position_embeddings,\n\u001b[0;32m    569\u001b[0m     )\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m    573\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    574\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    575\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    576\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    577\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    578\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    579\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    581\u001b[0m     )\n\u001b[0;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\models\\bitnet\\modeling_bitnet.py:279\u001b[0m, in \u001b[0;36mBitNetDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    280\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    281\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    282\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    283\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    284\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    285\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    286\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    287\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    289\u001b[0m )\n\u001b[0;32m    290\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\models\\bitnet\\modeling_bitnet.py:237\u001b[0m, in \u001b[0;36mBitNetAttention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 237\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    239\u001b[0m     query_states,\n\u001b[0;32m    240\u001b[0m     key_states,\n\u001b[0;32m    241\u001b[0m     value_states,\n\u001b[0;32m    242\u001b[0m     attention_mask,\n\u001b[0;32m    243\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    244\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    248\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    249\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_sub_norm(attn_output)  \u001b[38;5;66;03m# diff with Llama\u001b[39;00m\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:30\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msdpa_attention_forward\u001b[39m(\n\u001b[0;32m     19\u001b[0m     module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m     20\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         value \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m     33\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n",
      "File \u001b[1;32mb:\\Work\\Code\\p\\cag\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:15\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[1;34m(hidden_states, n_rep)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[0;32m     14\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_list, dataset = get_training_data(\"../input/training_data_1.json\")\n",
    "dataset = list(dataset)  # Convert the dataset to a list\n",
    "model_name = \"../model/bitnet-b1.58-2B-4T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cpu\", # <==== TODO change this\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "# max_questions = min(len(dataset), args.maxQuestion) if args.maxQuestion is not None else len(dataset)\n",
    "# Retrieve the knowledge from the vector database\n",
    "for id, (question, ground_truth) in enumerate(dataset[:]):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # Read the knowledge cache from the cache file\n",
    "    # cache_t1 = time()\n",
    "    # if args.kvcache == \"file\":\n",
    "    #     knowledge_cache = read_kv_cache(kvcache_path)\n",
    "\n",
    "    # Not a good idea to use this method, as it will consume a lot of memory\n",
    "    # if args.kvcache == \"variable\":\n",
    "    #     knowledge_cache = documents_cache\n",
    "    # cache_t2 = time()\n",
    "\n",
    "    # Generate Response for the question\n",
    "    knowledges = '\\n\\n\\n'.join(text_list)\n",
    "\n",
    "#     if args.usePrompt:\n",
    "#         prompt = f\"\"\"\n",
    "# <|begin_of_text|>\n",
    "# <|start_header_id|>system<|end_header_id|>\n",
    "# You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "# <|start_header_id|>user<|end_header_id|>\n",
    "# Context information is bellow.\n",
    "# ------------------------------------------------\n",
    "# {knowledges}\n",
    "# ------------------------------------------------\n",
    "# {answer_instruction}\n",
    "# Question:\n",
    "# {question}<|eot_id|>\n",
    "# <|start_header_id|>assistant<|end_header_id|>\n",
    "# \"\"\"\n",
    "#         generate_t1 = time()\n",
    "#         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#         output = generate(model, input_ids, DynamicCache()) \n",
    "#         generated_text = tokenizer.decode(output[0], skip_special_tokens=True, temperature=None)\n",
    "#         generate_t2 = time()\n",
    "#     else:\n",
    "    prompt = f\"\"\"\n",
    "        {question}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "    generate_t1 = time()\n",
    "    knowledge_cache = read_kv_cache(\"../data_cache/cache_knowledges_1.pt\")\n",
    "    kv_len = knowledge_cache.key_cache[0].shape[-2]\n",
    "    clean_up(knowledge_cache, kv_len)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = generate(model, input_ids, knowledge_cache)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True, temperature=None)\n",
    "    generate_t2 = time()\n",
    "\n",
    "    # print(\"D: \", knowledges)\n",
    "    print(\"Q: \", question)\n",
    "    print(\"A: \", generated_text)\n",
    "    print(time())\n",
    "\n",
    "#     # Evaluate bert-score similarity\n",
    "#     similarity = cagsim.bert(generated_text, ground_truth)\n",
    "\n",
    "#     print(f\"[{id}]: Semantic Similarity: {round(similarity, 5)},\",\n",
    "#             f\"cache time: {cache_t2 - cache_t1},\",\n",
    "#             f\"generate time: {generate_t2 - generate_t1}\")\n",
    "#     with open(args.output, \"a\") as f:\n",
    "#         f.write(f\"[{id}]: Semantic Similarity: {round(similarity, 5)},\\t cache time: {cache_t2 - cache_t1},\\t generate time: {generate_t2 - generate_t1}\\n\")\n",
    "\n",
    "#     results[\"prompts\"].append(question)\n",
    "#     results[\"responses\"].append(generated_text)\n",
    "#     results[\"cache_time\"].append(cache_t2 - cache_t1)\n",
    "#     results[\"generate_time\"].append(generate_t2 - generate_t1)\n",
    "#     results[\"similarity\"].append(similarity)\n",
    "\n",
    "#     with open(args.output, \"a\") as f:\n",
    "#         f.write(f\"[{id}]: [Cumulative]: \"\n",
    "#                 + f\"Semantic Similarity: {round(sum(results['similarity']) / (len(results['similarity'])) , 5)},\"\n",
    "#                 + f\"\\t cache time: {sum(results['cache_time']) / (len(results['cache_time'])) },\"\n",
    "#                 + f\"\\t generate time: {sum(results['generate_time']) / (len(results['generate_time'])) }\\n\")\n",
    "\n",
    "# avg_similarity = sum(results[\"similarity\"]) / len(results[\"similarity\"])\n",
    "# avg_cache_time = sum(results[\"cache_time\"]) / len(results[\"cache_time\"])\n",
    "# avg_generate_time = sum(results[\"generate_time\"]) / len(results[\"generate_time\"])\n",
    "# print()\n",
    "# print(f\"Prepare time: {prepare_time}\")\n",
    "# print(f\"Average Semantic Similarity: {avg_similarity}\")\n",
    "# print(f\"cache time: {avg_cache_time},\\t generate time: {avg_generate_time}\")\n",
    "# print()\n",
    "# with open(args.output, \"a\") as f:\n",
    "#     f.write(\"\\n\")\n",
    "#     f.write(f\"Result for {args.output}\\n\")\n",
    "#     f.write(f\"Prepare time: {prepare_time}\\n\")\n",
    "#     f.write(f\"Average Semantic Similarity: {avg_similarity}\\n\")\n",
    "#     f.write(f\"cache time: {avg_cache_time},\\t generate time: {avg_generate_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de32191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"../output/bitnet-b1.58-2B-4T\"\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830419d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Apply the chat template\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": '''Answer the following questions'''},\n",
    "#     {\"role\": \"user\", \"content\": \"What caused the rain's taste?\"},\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# chat_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate response\n",
    "# chat_outputs = model.generate(**chat_input, max_new_tokens=50)\n",
    "# response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True) # Decode only the response part\n",
    "# print(\"\\nAssistant Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d5c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
